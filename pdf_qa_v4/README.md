# DeepSeek-R1 PDF Q&A Assistant

## Overview
The **DeepSeek-R1 PDF Q&A Assistant** is a Streamlit-based web application that enables users to either upload a PDF document or search for academic papers by title, and then ask questions about their content. The application uses **DeepSeek-R1 (1.5B)**, an advanced language model, along with **HuggingFace embeddings** to index and retrieve relevant sections of the PDF for accurate responses.

## Features
- üìÑ **Two Ways to Access Documents**:
  - Direct PDF upload
  - Search papers by title (supports arXiv and Semantic Scholar)
- üîç **Smart Paper Search**:
  - Exact match search across academic repositories
  - Similar paper suggestions when exact match isn't found
  - Automatic paper download from supported sources
- ü§ñ **AI-Powered Q&A**: Answers user queries based on the document content using DeepSeek-R1
- üìä **Efficient Retrieval**: Uses **vector search** to find relevant sections of the document
- üñºÔ∏è **Non-Textual Element Support**: 
  - Automatically extracts and processes figures, tables, and charts
  - Generates descriptions for visual elements using multi-modal LLM
  - Enhances answers with references to relevant visual content
- üëÅÔ∏è **Visual Element Viewer**: Display extracted figures, tables, and charts in the sidebar
- üìú **PDF Preview**: Displays the document for reference
- üéõ **Reset Chat**: Clear previous interactions for a fresh start
- ‚ö° **Fast Response Generation**: Uses **streaming responses** for real-time answers
- üõ°Ô∏è **Robust Error Handling**: Graceful handling of connection issues and API changes

## Installation
### Prerequisites
- Python 3.9+
- Ollama installed and running
- 4GB+ free RAM
- Active internet connection for paper search functionality

1. **Install Ollama**:
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ollama pull deepseek-r1:1.5b
   ollama pull llava:7b  # For visual element analysis
   ```

### Steps to Install & Run
1. **Clone the repository**:
   ```bash
   git clone https://github.com/sadjad6/pdf_RAG_chatbot.git
   cd pdf_RAG_chatbot
   ```

2. **Create and activate a virtual environment** (optional but recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scripts\activate`
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the Streamlit application**:
   ```bash
   streamlit run pdf_qa_v4.py
   ```

## How It Works
1. **Choose Input Method**:
   - Upload a PDF directly via the sidebar
   - OR search for a paper by title using arXiv/Semantic Scholar integration
2. **Document Processing**:
   - For uploads: The document is directly indexed
   - For searches: The paper is downloaded and then indexed
3. **Document Indexing**:
   - Uses HuggingFace embeddings for efficient retrieval
   - Stores document sections for quick access
   - Extracts and processes non-textual elements (figures, tables, charts)
   - Generates descriptions for visual elements using LLaVA multi-modal model
4. **Question-Answering**:
   - Ask questions in the chat input field
   - DeepSeek-R1 retrieves relevant sections and generates responses
   - Includes information from both text and visual elements
   - View streaming responses in real-time

## Usage
1. **Select Input Method**:
   - Click "Upload PDF" and choose a file
   - OR select "Search by Title" and enter paper title
2. **For Paper Search**:
   - Enter the paper title
   - Select from exact matches or suggested similar papers
   - Wait for automatic download and processing
3. **Ask Questions**:
   - Enter your question in the chat input box
   - View the response generated by DeepSeek-R1
4. **View Visual Elements**:
   - Check the "Show Extracted Visual Elements" box
   - Explore figures, tables, and charts extracted from the document
   - View auto-generated descriptions of visual elements
5. **Additional Features**:
   - Click "Clear Chat" to reset the chat session
   - Preview the document in the sidebar

## Testing Visual Element Features
Try asking questions like:
- "Can you describe Figure 2 in the document?"
- "Summarize the data in Table 1."
- "What trend does the chart on page 5 show?"
- "How do the results in Table 2 relate to Figure 3?"
- "What visual evidence supports the main conclusion?"
- "Explain the relationship between the figures and the main findings of the paper."
- "Which tables contain information about the experiment results?"

## Troubleshooting
If you encounter any issues:

1. **Ollama Connection Errors**:
   - Ensure Ollama is running on your system
   - Verify that you've installed the required models (`deepseek-r1:1.5b` and `llava:7b`)
   - Check if port 11434 is accessible

2. **PDF Processing Issues**:
   - Some PDFs may have protection that prevents extraction of content
   - Complex layouts might lead to imperfect table or figure detection
   - Try with another PDF if extraction results are not satisfactory

3. **Memory Issues**:
   - Processing large PDFs with many images requires substantial memory
   - Close other memory-intensive applications
   - Consider reducing the PDF size if necessary

## Version History
- **v4 (Current)**: Added support for non-textual elements (figures, tables, charts), improved paper search, and enhanced error handling
- **v3**: Added academic paper search functionality
- **v2**: Improved PDF processing and context retrieval
- **v1**: Basic PDF question answering system

## Contributing
Feel free to fork the repository and submit pull requests. Contributions are welcome!

## License
This project is licensed under the MIT License. See the `LICENSE` file for more details.

---
üöÄ **Enjoy exploring your PDFs with AI!**